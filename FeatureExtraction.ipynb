{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import FunnelTokenizerFast, DataCollatorWithPadding, FunnelForSequenceClassification\n",
    "from ProjectDict import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# device = 'dml' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "print(torch.version.hip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpath = os.path.join(\"inspected_data\")\n",
    "theme = wonder_boy # laughing_man | wonder_boy\n",
    "theme_en = wonder_boy_en # laughing_man_en | wonder_boy_en\n",
    "model_type = model_types.base # model_types.base | model_types.fine_tuned\n",
    "df = pd.read_csv(os.path.join(rpath, f\"{theme}_splitted.csv\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for row in df.to_numpy():\n",
    "    text = \"\"\n",
    "    for sentence in row:\n",
    "        if type(sentence) == str:\n",
    "            text += '' + sentence\n",
    "    rows.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = df.to_numpy()\n",
    "len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.0.ffn.linear_1.weight']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_out.weight', 'classifier.linear_hidden.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FunnelForSequenceClassification(\n",
       "  (funnel): FunnelBaseModel(\n",
       "    (embeddings): FunnelEmbeddings(\n",
       "      (word_embeddings): Embedding(42000, 768)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): FunnelEncoder(\n",
       "      (attention_structure): FunnelAttentionStructure(\n",
       "        (sin_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (cos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x ModuleList(\n",
       "          (0-5): 6 x FunnelLayer(\n",
       "            (attention): FunnelRelMultiheadAttention(\n",
       "              (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_head): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k_head): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_head): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (post_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): FunnelPositionwiseFFN(\n",
       "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (activation_function): NewGELUActivation()\n",
       "              (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): FunnelClassificationHead(\n",
       "    (linear_hidden): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear_out): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_funnel = FunnelTokenizerFast.from_pretrained(\"kykim/funnel-kor-base\")\n",
    "if model_type == model_types.base:\n",
    "    model_funnel = FunnelForSequenceClassification.from_pretrained(\"kykim/funnel-kor-base\")\n",
    "else:\n",
    "    model_funnel = FunnelForSequenceClassification.from_pretrained(\"./FunnelLM/\")\n",
    "model_funnel = model_funnel.to(device)\n",
    "\n",
    "model_funnel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FunnelForSequenceClassification(\n",
      "  (funnel): FunnelBaseModel(\n",
      "    (embeddings): FunnelEmbeddings(\n",
      "      (word_embeddings): Embedding(42000, 768)\n",
      "      (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): FunnelEncoder(\n",
      "      (attention_structure): FunnelAttentionStructure(\n",
      "        (sin_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (cos_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0-2): 3 x ModuleList(\n",
      "          (0-5): 6 x FunnelLayer(\n",
      "            (attention): FunnelRelMultiheadAttention(\n",
      "              (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_head): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k_head): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_head): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (post_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
      "            )\n",
      "            (ffn): FunnelPositionwiseFFN(\n",
      "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (activation_function): NewGELUActivation()\n",
      "              (activation_dropout): Dropout(p=0.0, inplace=False)\n",
      "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): FunnelClassificationHead(\n",
      "    (linear_hidden): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear_out): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_funnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_funnel.classifier = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_in = []\n",
    "for str_arr in arr:\n",
    "    str_arr = [string for string in str_arr if type(string) == str]\n",
    "    pr_in.append(' '.join(str_arr))\n",
    "len(pr_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer_funnel, padding=True, max_length=tokenizer_funnel.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a FunnelTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\sizzf\\.conda\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    2, 39672,  8214,  ...,     0,     0,     0],\n",
       "        [    2, 31872, 19397,  ...,     0,     0,     0],\n",
       "        [    2,  2011, 23010,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2, 25341, 40661,  ...,     0,     0,     0],\n",
       "        [    2, 31872, 19397,  ...,     0,     0,     0],\n",
       "        [    2, 31872, 19397,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_in = tokenizer_funnel(pr_in, padding=True, truncation=True, max_length=tokenizer_funnel.model_max_length)\n",
    "collated_in = collator(tokenized_in)\n",
    "collated_in['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 125)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collated_in['input_ids'][0]), len(pr_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "N = len(pr_in)\n",
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n in tqdm(range(0+batch_size, N+batch_size-1, batch_size)):\n",
    "        start, end = max(0, n-batch_size), n\n",
    "        input_ids = collated_in['input_ids'][start:end].to(device)\n",
    "        attention_mask = collated_in['attention_mask'][start:end].to(device)\n",
    "        token_type_ids = collated_in['token_type_ids'][start:end].to(device)\n",
    "        embedding = model_funnel.forward(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).logits\n",
    "        # print(f\"{n}: {embedding}\")\n",
    "        embeddings.append(embedding)\n",
    "        del input_ids\n",
    "        del attention_mask\n",
    "        del token_type_ids\n",
    "        del embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.concat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings, f\"cache/{theme_en}_embeddings_{model_type}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e47abb3bc20041deac6632286b3b4b57a061d2b0d8fe9baeeda94211020ef32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
