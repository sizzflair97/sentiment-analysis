{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from ProjectDict import *\n",
    "from typing import Union\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regular_exp = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme = wonder_boy\n",
    "keyword: Union[str, None] = None\n",
    "ignore_split_process:bool = False\n",
    "\n",
    "rpath = os.path.join(os.getcwd(), \"inspected_data\")\n",
    "files = os.listdir(rpath)\n",
    "if keyword:\n",
    "    files = [f\"{theme}_{keyword}_inspected.csv\"]\n",
    "else:\n",
    "    files = [f\"{theme}_inspected.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['원더보이_inspected.csv']\n"
     ]
    }
   ],
   "source": [
    "print(files)\n",
    "assert len(files) == 1\n",
    "filename = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(rpath, filename))\n",
    "df = df.drop_duplicates(subset=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER_KEYWORDS = [\"잼\", \"재미\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "regex_strings = [url_regular_exp, r\"^nan \", r\"^RT \", r\"@[^\\s]*\", r\"[^\\x20-\\x7E\\uAC00-\\uD7A3]\",\n",
    "                 r'&...?;']\n",
    "re_list = [re.compile(regex) for regex in regex_strings]\n",
    "# re_list.append(re.compile(url_regular_exp)) # delete url\n",
    "# re_list.append(re.compile(r\"^nan \")) # delete RT\n",
    "# re_list.append(re.compile(r\"^RT \")) # delete RT\n",
    "# re_list.append(re.compile(r\"^@[^\\s]* \")) # delete mention text\n",
    "# re_list.append(re.compile(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\")) # delete specials\n",
    "\n",
    "strings = []\n",
    "\n",
    "for filename in files:\n",
    "    for title, text in zip(*map(pd.Series.to_list, [df['title'], df['text']])):\n",
    "        string = f\"{title} {text}\" # merge title and contents\n",
    "        string.replace('\\n', ' ')\n",
    "        for regex in re_list:\n",
    "            string = regex.sub(\"\", string)\n",
    "        string.strip()\n",
    "        strings.append(string)\n",
    "\n",
    "    #mask = [True if any((keyword in string) for keyword in FILTER_KEYWORDS) else False for string in strings]\n",
    "    #df = df[mask]\n",
    "    dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_hashtag_string = r\"\\[?#[^\\s]*\\]?\"\n",
    "regex_hashtag = re.compile(regex_hashtag_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하..'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiwi.split_into_sents(\"하.. 오늘 티켓팅으로 의욕이 너무 떨어졌다웃남 회전문 돌던 의욕 다 사라짐 이러고 또 취소표 미친듯이 잡겠지\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [[sent.text for sent in kiwi.split_into_sents(string)] for string in strings]\n",
    "\n",
    "if not ignore_split_process:\n",
    "    for splited in articles:\n",
    "        for i in range(len(splited)-1, -1, -1):\n",
    "            splited[i] = splited[i].strip()\n",
    "            # print(splited[i])\n",
    "            new_splited = []\n",
    "            split_points = set([0, len(splited[i])])\n",
    "            \n",
    "            for match in regex_hashtag.finditer(splited[i]):\n",
    "                # split_points.add(match.start())\n",
    "                split_points.add(match.start())\n",
    "                split_points.add(match.end())\n",
    "            split_points = sorted(list(split_points))\n",
    "            # print(split_points)\n",
    "            \n",
    "            for j in range(len(split_points)-1):\n",
    "                substring = splited[i][split_points[j]:split_points[j+1]].strip()\n",
    "                substring = substring.replace('#', '')\n",
    "                if substring != \"\":\n",
    "                    if substring[0] == '[' and substring[-1] == ']':\n",
    "                        substring = substring[1:-1]\n",
    "                    new_splited.append(substring)\n",
    "            \n",
    "            splited.pop(i)\n",
    "            for hashtag_splited in reversed(new_splited):\n",
    "                splited.insert(i, hashtag_splited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n",
      "3.584\n",
      "7.604910714285714\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "n_sentences = sum(len(article) for article in articles)\n",
    "mean_n_sentences = mean(len(article) for article in articles)\n",
    "mean_n_words_per_sentences = sum(sum(len(sentence.split()) for sentence in article) for article in articles) / n_sentences\n",
    "print(n_sentences)\n",
    "print(mean_n_sentences)\n",
    "print(mean_n_words_per_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "serieses = []\n",
    "for article in articles:\n",
    "    series = pd.Series(article)\n",
    "    serieses.append(series)\n",
    "\n",
    "inspected_df = pd.DataFrame(serieses)\n",
    "# inspected_df = pd.concat(serieses, axis=1).transpose()\n",
    "if keyword:\n",
    "    inspected_df.to_csv(os.path.join(rpath,f\"{theme}_{keyword}_splitted.csv\"), encoding='utf-8 sig', index=False, header=True)\n",
    "else:\n",
    "    inspected_df.to_csv(os.path.join(rpath,f\"{theme}_splitted.csv\"), encoding='utf-8 sig', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e47abb3bc20041deac6632286b3b4b57a061d2b0d8fe9baeeda94211020ef32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
