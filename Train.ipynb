{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch, datasets, evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import FunnelTokenizerFast, FunnelModel, Trainer, DataCollatorWithPadding, TrainingArguments, FunnelForSequenceClassification\n",
    "from time import time, localtime\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# device = 'dml' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "print(torch.version.hip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.0.attention.layer_norm.weight']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_out.weight', 'classifier.linear_hidden.bias', 'classifier.linear_out.bias', 'classifier.linear_hidden.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FunnelForSequenceClassification(\n",
       "  (funnel): FunnelBaseModel(\n",
       "    (embeddings): FunnelEmbeddings(\n",
       "      (word_embeddings): Embedding(42000, 768)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): FunnelEncoder(\n",
       "      (attention_structure): FunnelAttentionStructure(\n",
       "        (sin_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (cos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x ModuleList(\n",
       "          (0-5): 6 x FunnelLayer(\n",
       "            (attention): FunnelRelMultiheadAttention(\n",
       "              (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_head): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k_head): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_head): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (post_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): FunnelPositionwiseFFN(\n",
       "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (activation_function): NewGELUActivation()\n",
       "              (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): FunnelClassificationHead(\n",
       "    (linear_hidden): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear_out): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_funnel = FunnelTokenizerFast.from_pretrained(\"kykim/funnel-kor-base\")\n",
    "model_funnel = FunnelForSequenceClassification.from_pretrained(\"kykim/funnel-kor-base\", num_labels=2)\n",
    "model_funnel.train()\n",
    "model_funnel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Found cached dataset nsmc (C:/Users/sizzf/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d6da6ac5fd49d3923b8f849f8d9073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = datasets.load_dataset(\"nsmc\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9976970', 'document': '아 더빙.. 진짜 짜증나네요 목소리', 'label': 0}\n",
      "{'id': Value(dtype='string', id=None), 'document': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['negative', 'positive'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(raw_data['train'][0])\n",
    "print(raw_data['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(examples):\n",
    "    return tokenizer_funnel(examples[\"document\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/sizzf/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3\\cache-67863f3023fd0586.arrow\n",
      "Loading cached processed dataset at C:/Users/sizzf/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3\\cache-d49b264e64756d71.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_data.map(tokenizer_function, batched=True)\n",
    "print(tokenized_datasets)\n",
    "print(tokenizer_funnel.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer_funnel, padding=True)\n",
    "training_args = TrainingArguments(\"test_trainer\", learning_rate=3e-5, num_train_epochs=15, per_device_train_batch_size=batch_size, no_cuda=False, save_strategy=\"epoch\")\n",
    "metric = evaluate.load(\"precision\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = tokenized_datasets[\"train\"]\n",
    "test_samples = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model_funnel,\n",
    "                  training_args,\n",
    "                  train_dataset=train_samples,\n",
    "                  eval_dataset=test_samples,\n",
    "                  data_collator=collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer_funnel)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./FunnelLM-1-11-9-13-5\\config.json\n",
      "Model weights saved in ./FunnelLM-1-11-9-13-5\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "lt = localtime(time())\n",
    "model_funnel.save_pretrained(f\"./FunnelLM-{lt.tm_mon}-{lt.tm_mday}-{lt.tm_hour}-{lt.tm_min}-{lt.tm_sec}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e47abb3bc20041deac6632286b3b4b57a061d2b0d8fe9baeeda94211020ef32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
